import { supabaseAdmin } from '@/lib/supabase'
import { google } from 'googleapis'
import fs from 'fs'
import path from 'path'
import { format } from 'date-fns'

/**
 * PropMaster 3.0 - Comprehensive Database Backup System
 * 
 * Features:
 * - Automated daily/weekly/monthly backups
 * - Google Drive integration for secure cloud storage
 * - Point-in-time recovery capabilities
 * - Backup verification and integrity checks
 * - Selective table backups
 * - Compression and encryption options
 */

export class BackupSystem {
  constructor(options = {}) {
    this.supabase = supabaseAdmin
    this.options = {
      compressionLevel: 9,
      encryptBackups: true,
      maxBackupAge: 90, // days
      maxBackupsPerType: 10,
      ...options
    }
    
    // Initialize Google Drive API
    this.driveAuth = null
    this.drive = null
    this.initializeGoogleDrive()
  }

  /**
   * Initialize Google Drive API client
   */
  async initializeGoogleDrive() {
    try {
      // Load service account credentials from environment
      const credentials = {
        type: 'service_account',
        project_id: process.env.GOOGLE_CLOUD_PROJECT_ID,
        private_key_id: process.env.GOOGLE_CLOUD_PRIVATE_KEY_ID,
        private_key: process.env.GOOGLE_CLOUD_PRIVATE_KEY?.replace(/\\n/g, '\n'),
        client_email: process.env.GOOGLE_CLOUD_CLIENT_EMAIL,
        client_id: process.env.GOOGLE_CLOUD_CLIENT_ID,
        auth_uri: 'https://accounts.google.com/o/oauth2/auth',
        token_uri: 'https://oauth2.googleapis.com/token'
      }

      if (!credentials.private_key || !credentials.client_email) {
        console.warn('Google Drive credentials not configured. Backups will be stored locally only.')
        return
      }

      this.driveAuth = new google.auth.GoogleAuth({
        credentials,
        scopes: ['https://www.googleapis.com/auth/drive.file']
      })

      this.drive = google.drive({ 
        version: 'v3', 
        auth: this.driveAuth 
      })

      console.log('‚úÖ Google Drive API initialized successfully')
    } catch (error) {
      console.error('‚ùå Failed to initialize Google Drive API:', error)
      this.drive = null
    }
  }

  /**
   * Create a comprehensive database backup
   */
  async createFullBackup(options = {}) {
    const {
      backupType = 'manual',
      includeFiles = true,
      uploadToDrive = true,
      verifyBackup = true
    } = options

    const timestamp = format(new Date(), 'yyyy-MM-dd_HH-mm-ss')
    const backupId = `propmaster_backup_${timestamp}`
    
    console.log(`üöÄ Starting ${backupType} backup: ${backupId}`)

    const backup = {
      id: backupId,
      timestamp: new Date().toISOString(),
      type: backupType,
      status: 'in_progress',
      tables: {},
      files: [],
      metadata: {
        version: '3.0',
        created_by: 'PropMaster Backup System',
        database_url: process.env.NEXT_PUBLIC_SUPABASE_URL,
        total_size: 0,
        compressed_size: 0
      },
      verification: null,
      storage: {
        local_path: null,
        drive_file_id: null,
        drive_folder_id: process.env.GOOGLE_DRIVE_BACKUP_FOLDER_ID
      }
    }

    try {
      // Step 1: Export all database tables
      console.log('üìä Exporting database tables...')
      
      const tables = ['users', 'properties', 'finances', 'client_profiles']
      
      for (const tableName of tables) {
        try {
          const tableBackup = await this.exportTable(tableName)
          backup.tables[tableName] = tableBackup
          console.log(`‚úÖ Exported ${tableName}: ${tableBackup.record_count} records`)
        } catch (error) {
          console.error(`‚ùå Failed to export ${tableName}:`, error)
          backup.tables[tableName] = {
            success: false,
            error: error.message,
            record_count: 0,
            data: null
          }
        }
      }

      // Step 2: Export file attachments (if enabled)
      if (includeFiles) {
        console.log('üìÅ Exporting file attachments...')
        backup.files = await this.exportFiles()
      }

      // Step 3: Create backup package
      console.log('üì¶ Creating backup package...')
      const backupPath = await this.createBackupPackage(backup)
      backup.storage.local_path = backupPath

      // Step 4: Upload to Google Drive (if enabled and configured)
      if (uploadToDrive && this.drive) {
        console.log('‚òÅÔ∏è Uploading to Google Drive...')
        const driveFileId = await this.uploadToGoogleDrive(backupPath, backup)
        backup.storage.drive_file_id = driveFileId
      }

      // Step 5: Verify backup integrity (if enabled)
      if (verifyBackup) {
        console.log('üîç Verifying backup integrity...')
        backup.verification = await this.verifyBackup(backup)
      }

      // Step 6: Update backup status and cleanup
      backup.status = 'completed'
      backup.completed_at = new Date().toISOString()
      
      // Save backup metadata
      await this.saveBackupMetadata(backup)
      
      // Cleanup old backups
      await this.cleanupOldBackups(backupType)

      console.log(`‚úÖ Backup completed successfully: ${backupId}`)
      return backup

    } catch (error) {
      console.error(`‚ùå Backup failed: ${error.message}`)
      backup.status = 'failed'
      backup.error = error.message
      backup.completed_at = new Date().toISOString()
      
      await this.saveBackupMetadata(backup)
      throw error
    }
  }

  /**
   * Export a single database table
   */
  async exportTable(tableName) {
    try {
      const { data, error, count } = await this.supabase
        .from(tableName)
        .select('*', { count: 'exact' })

      if (error) {
        throw new Error(`Database export error: ${error.message}`)
      }

      return {
        success: true,
        record_count: count || 0,
        data: data || [],
        exported_at: new Date().toISOString(),
        checksum: this.calculateChecksum(JSON.stringify(data))
      }
    } catch (error) {
      throw new Error(`Failed to export ${tableName}: ${error.message}`)
    }
  }

  /**
   * Export file attachments from Supabase Storage
   */
  async exportFiles() {
    const files = []
    
    try {
      // Get list of all files in storage buckets
      const buckets = ['property-images', 'property-documents', 'receipts']
      
      for (const bucketName of buckets) {
        try {
          const { data: bucketFiles, error } = await this.supabase.storage
            .from(bucketName)
            .list()

          if (error) {
            console.warn(`Warning: Could not access bucket ${bucketName}:`, error.message)
            continue
          }

          for (const file of bucketFiles || []) {
            try {
              const { data: fileData, error: downloadError } = await this.supabase.storage
                .from(bucketName)
                .download(file.name)

              if (!downloadError && fileData) {
                files.push({
                  bucket: bucketName,
                  name: file.name,
                  size: file.metadata?.size || 0,
                  last_modified: file.updated_at,
                  data: await fileData.arrayBuffer()
                })
              }
            } catch (fileError) {
              console.warn(`Warning: Could not download ${file.name}:`, fileError.message)
            }
          }
        } catch (bucketError) {
          console.warn(`Warning: Could not list files in ${bucketName}:`, bucketError.message)
        }
      }

      console.log(`üìÅ Exported ${files.length} files`)
      return files
    } catch (error) {
      console.error('Error exporting files:', error)
      return []
    }
  }

  /**
   * Create compressed backup package
   */
  async createBackupPackage(backup) {
    const zlib = await import('zlib')
    const backupDir = path.join(process.cwd(), 'backups')
    
    // Ensure backup directory exists
    if (!fs.existsSync(backupDir)) {
      fs.mkdirSync(backupDir, { recursive: true })
    }

    const backupPath = path.join(backupDir, `${backup.id}.json.gz`)
    
    // Create compressed backup file
    const backupData = JSON.stringify(backup, null, 2)
    const compressed = zlib.gzipSync(backupData, { level: this.options.compressionLevel })
    
    fs.writeFileSync(backupPath, compressed)
    
    // Update metadata
    backup.metadata.total_size = Buffer.byteLength(backupData)
    backup.metadata.compressed_size = compressed.length
    backup.metadata.compression_ratio = (compressed.length / backup.metadata.total_size * 100).toFixed(2)

    console.log(`üì¶ Backup package created: ${backupPath}`)
    console.log(`üíæ Size: ${this.formatBytes(backup.metadata.total_size)} ‚Üí ${this.formatBytes(compressed.length)} (${backup.metadata.compression_ratio}%)`)
    
    return backupPath
  }

  /**
   * Upload backup to Google Drive
   */
  async uploadToGoogleDrive(backupPath, backup) {
    if (!this.drive) {
      throw new Error('Google Drive not configured')
    }

    try {
      const fileMetadata = {
        name: `${backup.id}.json.gz`,
        parents: backup.storage.drive_folder_id ? [backup.storage.drive_folder_id] : undefined,
        description: `PropMaster 3.0 Database Backup - ${backup.type} - ${backup.timestamp}`
      }

      const media = {
        mimeType: 'application/gzip',
        body: fs.createReadStream(backupPath)
      }

      const response = await this.drive.files.create({
        requestBody: fileMetadata,
        media: media,
        fields: 'id,name,size,createdTime'
      })

      console.log(`‚òÅÔ∏è Uploaded to Google Drive: ${response.data.name} (${response.data.id})`)
      return response.data.id
    } catch (error) {
      console.error('Failed to upload to Google Drive:', error)
      throw error
    }
  }

  /**
   * Verify backup integrity
   */
  async verifyBackup(backup) {
    const verification = {
      timestamp: new Date().toISOString(),
      status: 'unknown',
      checks: {},
      issues: []
    }

    try {
      // Check 1: Verify table data integrity
      let totalRecords = 0
      let tablesWithData = 0
      
      for (const [tableName, tableBackup] of Object.entries(backup.tables)) {
        const check = {
          has_data: tableBackup.success && tableBackup.record_count > 0,
          record_count: tableBackup.record_count || 0,
          checksum_valid: true
        }

        if (tableBackup.success && tableBackup.data) {
          const currentChecksum = this.calculateChecksum(JSON.stringify(tableBackup.data))
          check.checksum_valid = currentChecksum === tableBackup.checksum
          
          if (!check.checksum_valid) {
            verification.issues.push(`Checksum mismatch for table ${tableName}`)
          }
        }

        verification.checks[tableName] = check
        totalRecords += check.record_count
        
        if (check.has_data) tablesWithData++
      }

      // Check 2: Verify backup file exists and is readable
      if (backup.storage.local_path && fs.existsSync(backup.storage.local_path)) {
        const stats = fs.statSync(backup.storage.local_path)
        verification.checks.backup_file = {
          exists: true,
          size: stats.size,
          readable: true
        }
      } else {
        verification.checks.backup_file = {
          exists: false,
          size: 0,
          readable: false
        }
        verification.issues.push('Backup file not found or not accessible')
      }

      // Check 3: Verify Google Drive upload (if applicable)
      if (backup.storage.drive_file_id && this.drive) {
        try {
          const driveFile = await this.drive.files.get({
            fileId: backup.storage.drive_file_id,
            fields: 'id,name,size,createdTime'
          })
          
          verification.checks.google_drive = {
            uploaded: true,
            file_id: driveFile.data.id,
            size: parseInt(driveFile.data.size) || 0
          }
        } catch (driveError) {
          verification.checks.google_drive = {
            uploaded: false,
            error: driveError.message
          }
          verification.issues.push(`Google Drive verification failed: ${driveError.message}`)
        }
      }

      // Determine overall status
      verification.summary = {
        total_records: totalRecords,
        tables_with_data: tablesWithData,
        total_tables: Object.keys(backup.tables).length,
        files_count: backup.files?.length || 0,
        issues_count: verification.issues.length
      }

      if (verification.issues.length === 0 && totalRecords > 0) {
        verification.status = 'valid'
      } else if (verification.issues.length > 0 && totalRecords > 0) {
        verification.status = 'valid_with_warnings'
      } else {
        verification.status = 'invalid'
      }

      console.log(`üîç Backup verification: ${verification.status} (${totalRecords} records, ${verification.issues.length} issues)`)
      return verification
    } catch (error) {
      verification.status = 'error'
      verification.error = error.message
      verification.issues.push(`Verification failed: ${error.message}`)
      return verification
    }
  }

  /**
   * Save backup metadata to local storage
   */
  async saveBackupMetadata(backup) {
    const metadataDir = path.join(process.cwd(), 'backups', 'metadata')
    
    if (!fs.existsSync(metadataDir)) {
      fs.mkdirSync(metadataDir, { recursive: true })
    }

    const metadataPath = path.join(metadataDir, `${backup.id}.json`)
    
    // Create lightweight metadata (without actual data)
    const metadata = {
      ...backup,
      tables: Object.fromEntries(
        Object.entries(backup.tables).map(([name, table]) => [
          name, 
          { ...table, data: table.data ? '[DATA_REMOVED]' : null }
        ])
      ),
      files: backup.files?.map(file => ({
        ...file,
        data: file.data ? '[FILE_DATA_REMOVED]' : null
      })) || []
    }

    fs.writeFileSync(metadataPath, JSON.stringify(metadata, null, 2))
    console.log(`üíæ Backup metadata saved: ${metadataPath}`)
  }

  /**
   * Clean up old backups based on retention policy
   */
  async cleanupOldBackups(backupType) {
    try {
      const backupDir = path.join(process.cwd(), 'backups')
      const metadataDir = path.join(backupDir, 'metadata')
      
      if (!fs.existsSync(metadataDir)) return

      // Get all backup metadata files
      const metadataFiles = fs.readdirSync(metadataDir)
        .filter(file => file.endsWith('.json'))
        .map(file => {
          const filePath = path.join(metadataDir, file)
          const metadata = JSON.parse(fs.readFileSync(filePath, 'utf8'))
          return { ...metadata, metadataPath: filePath }
        })
        .filter(backup => backup.type === backupType)
        .sort((a, b) => new Date(b.timestamp) - new Date(a.timestamp))

      // Keep only the most recent backups
      const backupsToDelete = metadataFiles.slice(this.options.maxBackupsPerType)
      
      for (const backup of backupsToDelete) {
        try {
          // Delete local backup file
          if (backup.storage.local_path && fs.existsSync(backup.storage.local_path)) {
            fs.unlinkSync(backup.storage.local_path)
            console.log(`üóëÔ∏è Deleted local backup: ${backup.storage.local_path}`)
          }

          // Delete from Google Drive
          if (backup.storage.drive_file_id && this.drive) {
            await this.drive.files.delete({ fileId: backup.storage.drive_file_id })
            console.log(`üóëÔ∏è Deleted from Google Drive: ${backup.storage.drive_file_id}`)
          }

          // Delete metadata file
          if (fs.existsSync(backup.metadataPath)) {
            fs.unlinkSync(backup.metadataPath)
            console.log(`üóëÔ∏è Deleted metadata: ${backup.metadataPath}`)
          }
        } catch (deleteError) {
          console.error(`Warning: Failed to delete backup ${backup.id}:`, deleteError.message)
        }
      }

      if (backupsToDelete.length > 0) {
        console.log(`üßπ Cleaned up ${backupsToDelete.length} old ${backupType} backups`)
      }
    } catch (error) {
      console.error('Error during backup cleanup:', error)
    }
  }

  /**
   * List all available backups
   */
  async listBackups(options = {}) {
    const { backupType, limit = 50 } = options
    
    try {
      const metadataDir = path.join(process.cwd(), 'backups', 'metadata')
      
      if (!fs.existsSync(metadataDir)) {
        return []
      }

      const backups = fs.readdirSync(metadataDir)
        .filter(file => file.endsWith('.json'))
        .map(file => {
          const filePath = path.join(metadataDir, file)
          const metadata = JSON.parse(fs.readFileSync(filePath, 'utf8'))
          return metadata
        })
        .filter(backup => !backupType || backup.type === backupType)
        .sort((a, b) => new Date(b.timestamp) - new Date(a.timestamp))
        .slice(0, limit)

      return backups
    } catch (error) {
      console.error('Error listing backups:', error)
      return []
    }
  }

  /**
   * Restore database from backup
   */
  async restoreFromBackup(backupId, options = {}) {
    const { 
      tables = null, // null = restore all tables
      confirmRestore = false,
      createBackupBeforeRestore = true 
    } = options

    if (!confirmRestore) {
      throw new Error('Restore operation requires explicit confirmation (set confirmRestore: true)')
    }

    console.log(`üîÑ Starting restore from backup: ${backupId}`)

    try {
      // Step 1: Load backup data
      const backup = await this.loadBackup(backupId)
      if (!backup) {
        throw new Error(`Backup not found: ${backupId}`)
      }

      // Step 2: Create safety backup before restore (if enabled)
      if (createBackupBeforeRestore) {
        console.log('üíæ Creating safety backup before restore...')
        await this.createFullBackup({ 
          backupType: 'pre_restore_safety',
          uploadToDrive: false 
        })
      }

      // Step 3: Restore tables
      const tablesToRestore = tables || Object.keys(backup.tables)
      const restoreResults = {}

      for (const tableName of tablesToRestore) {
        if (!backup.tables[tableName] || !backup.tables[tableName].success) {
          restoreResults[tableName] = {
            success: false,
            error: 'Table data not available in backup'
          }
          continue
        }

        try {
          const result = await this.restoreTable(tableName, backup.tables[tableName].data)
          restoreResults[tableName] = result
          console.log(`‚úÖ Restored ${tableName}: ${result.records_restored} records`)
        } catch (error) {
          restoreResults[tableName] = {
            success: false,
            error: error.message
          }
          console.error(`‚ùå Failed to restore ${tableName}:`, error)
        }
      }

      const successfulRestores = Object.values(restoreResults).filter(r => r.success).length
      const totalRestores = Object.keys(restoreResults).length

      console.log(`‚úÖ Restore completed: ${successfulRestores}/${totalRestores} tables restored`)

      return {
        success: successfulRestores > 0,
        backup_id: backupId,
        restored_tables: successfulRestores,
        total_tables: totalRestores,
        results: restoreResults,
        restored_at: new Date().toISOString()
      }
    } catch (error) {
      console.error(`‚ùå Restore failed:`, error)
      throw error
    }
  }

  /**
   * Load backup data from file
   */
  async loadBackup(backupId) {
    const zlib = await import('zlib')
    const backupPath = path.join(process.cwd(), 'backups', `${backupId}.json.gz`)
    
    if (!fs.existsSync(backupPath)) {
      return null
    }

    try {
      const compressed = fs.readFileSync(backupPath)
      const decompressed = zlib.gunzipSync(compressed)
      const backup = JSON.parse(decompressed.toString())
      return backup
    } catch (error) {
      console.error(`Failed to load backup ${backupId}:`, error)
      return null
    }
  }

  /**
   * Restore a single table from backup data
   */
  async restoreTable(tableName, tableData) {
    if (!Array.isArray(tableData)) {
      throw new Error(`Invalid table data for ${tableName}`)
    }

    try {
      // Step 1: Clear existing data (with confirmation)
      const { error: deleteError } = await this.supabase
        .from(tableName)
        .delete()
        .neq('id', '00000000-0000-0000-0000-000000000000') // Delete all records

      if (deleteError) {
        console.warn(`Warning: Could not clear ${tableName}:`, deleteError.message)
      }

      // Step 2: Insert backup data in batches
      const batchSize = 100
      let totalRestored = 0

      for (let i = 0; i < tableData.length; i += batchSize) {
        const batch = tableData.slice(i, i + batchSize)
        
        const { data, error } = await this.supabase
          .from(tableName)
          .insert(batch)

        if (error) {
          console.error(`Batch insert error for ${tableName}:`, error)
          // Continue with next batch instead of failing completely
        } else {
          totalRestored += batch.length
        }
      }

      return {
        success: true,
        records_restored: totalRestored,
        total_records: tableData.length
      }
    } catch (error) {
      throw new Error(`Failed to restore ${tableName}: ${error.message}`)
    }
  }

  /**
   * Schedule automated backups
   */
  async scheduleBackups() {
    // This would typically be called by a cron job or scheduled task
    const now = new Date()
    const hour = now.getHours()
    const day = now.getDay()
    const date = now.getDate()

    try {
      // Daily backup at 2 AM
      if (hour === 2) {
        await this.createFullBackup({
          backupType: 'daily',
          includeFiles: false, // Skip files for daily backups to save space
          uploadToDrive: true,
          verifyBackup: true
        })
      }

      // Weekly backup on Sunday at 3 AM
      if (day === 0 && hour === 3) {
        await this.createFullBackup({
          backupType: 'weekly',
          includeFiles: true,
          uploadToDrive: true,
          verifyBackup: true
        })
      }

      // Monthly backup on 1st day at 4 AM
      if (date === 1 && hour === 4) {
        await this.createFullBackup({
          backupType: 'monthly',
          includeFiles: true,
          uploadToDrive: true,
          verifyBackup: true
        })
      }
    } catch (error) {
      console.error('Scheduled backup failed:', error)
      // Could send alert/notification here
    }
  }

  /**
   * Get backup system health and statistics
   */
  async getBackupHealth() {
    try {
      const backups = await this.listBackups({ limit: 100 })
      const now = new Date()
      const oneDayAgo = new Date(now.getTime() - 24 * 60 * 60 * 1000)
      const oneWeekAgo = new Date(now.getTime() - 7 * 24 * 60 * 60 * 1000)

      const recentBackups = backups.filter(b => new Date(b.timestamp) > oneDayAgo)
      const weeklyBackups = backups.filter(b => new Date(b.timestamp) > oneWeekAgo)
      const failedBackups = backups.filter(b => b.status === 'failed')

      const health = {
        status: 'healthy',
        last_backup: backups[0]?.timestamp || null,
        total_backups: backups.length,
        recent_backups: recentBackups.length,
        weekly_backups: weeklyBackups.length,
        failed_backups: failedBackups.length,
        google_drive_configured: !!this.drive,
        storage_usage: {
          local: this.calculateLocalStorageUsage(),
          drive: null // Would need to query Drive API
        },
        recommendations: []
      }

      // Health checks and recommendations
      if (!health.last_backup) {
        health.status = 'critical'
        health.recommendations.push('No backups found - create your first backup')
      } else if (new Date(health.last_backup) < oneWeekAgo) {
        health.status = 'warning'
        health.recommendations.push('Last backup is over a week old - consider more frequent backups')
      }

      if (!health.google_drive_configured) {
        health.recommendations.push('Configure Google Drive for secure cloud backups')
      }

      if (health.failed_backups > 0) {
        health.recommendations.push(`${health.failed_backups} recent backups failed - check error logs`)
      }

      return health
    } catch (error) {
      return {
        status: 'error',
        error: error.message,
        recommendations: ['Check backup system configuration']
      }
    }
  }

  /**
   * Utility functions
   */
  calculateChecksum(data) {
    const crypto = require('crypto')
    return crypto.createHash('md5').update(data).digest('hex')
  }

  formatBytes(bytes) {
    if (bytes === 0) return '0 Bytes'
    const k = 1024
    const sizes = ['Bytes', 'KB', 'MB', 'GB']
    const i = Math.floor(Math.log(bytes) / Math.log(k))
    return parseFloat((bytes / Math.pow(k, i)).toFixed(2)) + ' ' + sizes[i]
  }

  calculateLocalStorageUsage() {
    try {
      const backupDir = path.join(process.cwd(), 'backups')
      if (!fs.existsSync(backupDir)) return 0

      let totalSize = 0
      const files = fs.readdirSync(backupDir, { recursive: true })
      
      for (const file of files) {
        const filePath = path.join(backupDir, file)
        if (fs.statSync(filePath).isFile()) {
          totalSize += fs.statSync(filePath).size
        }
      }

      return totalSize
    } catch (error) {
      return 0
    }
  }
}

// Export singleton instance
export const backupSystem = new BackupSystem()
